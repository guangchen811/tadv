from inspect import cleandoc

SYSTEM_TASK_DESCRIPTION = cleandoc("""Context-aware data validation aims to generate data validation constraints on the dataset where the downstream task is run. "Context" here refers to the downstream code. The system should generate effective and efficient constraints on the data.
*Effective*: The constraints should reflect whether the data is harmful to the downstream code. Constraints should fail if the data is harmful to the downstream code. Additionally, the constraints should be precise to reduce false positive alerts, meaning the constraints should not fail if the data is not harmful to the downstream task.
*Efficient*: The constraints should only be applied to relevant columns used by the code. Constraints on unused columns would lead to redundant calculations and false positive alerts.

The context-aware data validation system consists of the following components:
1. *Relevant Column Target*:
Input: 
    Statistics of the columns in the dataset.
    Downstream code snippet.
Output:
    Relevant columns used by the downstream code snippet.
Given a dataset and the downstream code, this component finds the columns that are used in the code snippet. These columns are the relevant columns for the downstream task.
2. *Assumptions Extraction*:
Input:
    Statistics of the columns in the dataset.
    Relevant columns used by the downstream code snippet.
    Downstream code snippet.
Output:
    User's assumptions about the relevant columns.
Given the relevant columns, the dataset statistics, and the downstream code, this component extracts the code's assumptions and requirements about the relevant columns. These assumptions are used to generate executable validation rules, ensuring the data meets the user's expectations.
3. *Rule Generation*:
Input:
    Statistics of the columns in the dataset.
    Relevant columns used by the downstream code snippet.
    Code's assumptions about the relevant columns.
    Downstream code snippet.
Output:
    Validation rules in PyDeequ format.
Given a dataset described by name, columns with corresponding downstream code, the relevant columns and the user's assumptions. This component converts the code's assumptions and requirements into formal validation rules to ensure the data meets the code's expectations. The rules are generated in PyDeequ format.
""")

RELEVANT_COLUMN_TARGET_PROMPT = cleandoc("""You are part of the context-aware data validation system. You serve as the *Relevant Column Target* component.
Given a dataset and the downstream code, you are asked to find the columns that are used in the code snippet. These columns are the relevant columns for the downstream task to ensure that the constraints are only applied to relevant columns.

The dataset is a CSV file with the following columns:
{columns_desc}

The user writes the code snippet below:
{code_snippet}

The above code snippet is used for the following downstream task:
{downstream_task_description}

Your response should be a list of comma separated values
eg: `foo, bar, baz` or `foo,bar,baz`
""")

# https://github.com/awslabs/python-deequ/blob/master/pydeequ/checks.py
ASSUMPTIONS_EXTRACTION_PROMPT = cleandoc("""You are part of a context-aware data validation system. You serve as the *Assumptions Extraction* component.
Given that the code written for the downstream task may be not robust enough to handle all possible data scenarios, you should find the code's assumptions and requirements about the relevant columns. These assumptions would then be used to generate validation rules to ensure the data meets the code's expectations and requirements.


Here are the things you need to consider:
1. Make sure the assumptions are only about the relevant columns generated by the *Relevant Column Target* component.
2. You can generate assumptions for both individual columns, i.e., the column named 'age' should be greater than 18, and relationships between columns, i.e., the column named 'age' should be greater than the column named 'min_age'.
3. The assumptions should be in a human-readable format and would be converted into formal validation rules in the next step.

The dataset is a CSV file with the following columns:
{columns_desc}

The user writes the code snippet below:
{code_snippet}

The relevant columns generated by the *Relevant Column Target* component are:
{relevant_columns}

The above code snippet is used for the following downstream task:
{downstream_task_description}

Please generate validation rules as a JSON object with the column names as keys and a list of assumptions as values.
e.g., ```{{'column_name_1': ['assumption_1', 'assumption_2', ...], 'column_name_2': ['assumption_1', 'assumption_2', ...], ...}}```
""")

RULE_GENERATION_PROMPT = cleandoc("""You are part of a context-aware data validation system. You serve as the *Rule Generation* component.
You are asked to transform the code's assumptions into formal validation rules to ensure the data meets the user's assumptions and requirements. The rules should be generated in PyDeequ format.

The function signature for PyDeequ constraints is as follows:
    hasSize(assertion)
    isComplete(column)
    hasCompleteness(column, assertion)
    areComplete(columns)
    haveCompleteness(columns, assertion)
    areAnyComplete(columns)
    haveAnyCompleteness(columns, assertion)
    isUnique(column)
    isPrimaryKey(column, *columns)
    hasUniqueness(columns, assertion)
    hasDistinctness(columns, assertion)
    hasUniqueValueRatio(columns, assertion)
    hasNumberOfDistinctValues(column, assertion, binningUdf, maxBins)
    hasHistogramValues(column, assertion, binningUdf, maxBins)
    hasEntropy(column, assertion)
    hasMutualInformation(columnA, columnB, assertion)
    hasApproxQuantile(column, quantile, assertion)
    hasMinLength(column, assertion)
    hasMaxLength(column, assertion)
    hasMin(column, assertion)
    hasMax(column, assertion)
    hasMean(column, assertion)
    hasSum(column, assertion)
    hasStandardDeviation(column, assertion)
    hasApproxCountDistinct(column, assertion)
    hasCorrelation(columnA, columnB, assertion)
    satisfies(columnCondition, constraintName, assertion=None)
    hasPattern(column, pattern, assertion=None, name=None)
    containsCreditCardNumber(column, assertion=None)
    containsEmail(column, assertion=None)
    containsURL(column, assertion=None)
    containsSocialSecurityNumber(column, assertion=None)
    isNonNegative(column, assertion=None)
    isPositive(column, assertion=None)
    isLessThan(columnA, columnB, assertion=None)
    isLessThanOrEqualTo(columnA, columnB, assertion=None)
    isGreaterThan(columnA, columnB, assertion=None)
    isGreaterThanOrEqualTo(columnA, columnB, assertion=None)
    isContainedIn(column, allowed_values, assertion=None)
    
The assertions should be a lambda function that returns a boolean value. For example, `lambda x: x > 18` or `lambda x: x == 1.0`. You don't need to provide the assertion variable if it is optional.

To help you understand the signature, here are some examples:

    .hasMin('person_age', lambda x: x > 18)
    .hasMax('person_age', lambda x: x < 120)
    .isComplete('loan_status')
    .hasCompleteness('loan_status', lambda x: x == 1.0)
    .isUnique('id')
    .hasUniqueValueRatio(['id'], lambda x: x > 0.8)
    .hasEntropy('loan_status', lambda x: x > 0.4)
    .hasMutualInformation('loan_grade', 'loan_amnt', lambda x: x < 0.1)
    .hasApproxQuantile('person_income', 0.5, lambda x: x > 0.8)
    .hasMinLength('loan_intent', lambda x: x > 1)
    .hasMaxLength('loan_intent', lambda x: x < 20)
    .hasStandardDeviation('person_income', lambda x: x > 0.8)
    .hasApproxCountDistinct('loan_intent', lambda x: x > 0.8)
    .hasCorrelation('person_income', 'loan_amnt', lambda x: x > 0.3)
    .satisfies('person_income > 0 WHERE loan_amnt > 0', lambda x: x > 0.8)
    .hasPattern('person_home_ownership', 'RENT|OWN|MORTGAGE|OTHER', lambda x: x > 0.8)
    .isContainedIn('loan_grade', ['A', 'B', 'C', 'D', 'E', 'F', 'G'])
    .containsURL('loan_intent', lambda x: x == 0)
    .isPositive('person_income')
    .isGreaterThan('person_income', 'loan_amnt', lambda x: x > 0.8)

The user writes the code snippet below:
{code_snippet}

The above code snippet is used for the following downstream task:
{downstream_task_description}

The relevant columns generated by the *Relevant Column Target* component are:
{relevant_columns}

The Intuitions generated by the *Assumption Extraction* component are:
{assumptions}


Please generate validation rules as a JSON object with the column names as keys and a list of assumptions as values.
e.g., ```
{{
    "column_name_1": ["pydeequ_constraint_1", "pydeequ_constraint_2", ...],
    "column_name_2": ["pydeequ_constraint_1", "pydeequ_constraint_2", ...],
    ...
}}```
The pydeequ_constraint_i should be replaced with the PyDeequ constraints, for example, `.isComplete("column_name")` or `.isContainedIn("column_name", ["value_1", "value_2"])`.
""")
